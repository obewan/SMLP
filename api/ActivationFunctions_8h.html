<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>SMLP: /home/runner/work/SMLP/SMLP/libs/libsmlp/include/ActivationFunctions.h File Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">SMLP
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="dir_6719ab1f1f7655efc2fa43f7eb574fd1.html">libs</a></li><li class="navelem"><a class="el" href="dir_d08ea8038c746a924269aa172a8e83d0.html">libsmlp</a></li><li class="navelem"><a class="el" href="dir_a294c0766837cdef758c57033b671e19.html">include</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#namespaces">Namespaces</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle">
<div class="title">ActivationFunctions.h File Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>Activation functions.  
<a href="#details">More...</a></p>
<div class="textblock"><code>#include &lt;math.h&gt;</code><br />
</div>
<p><a href="ActivationFunctions_8h_source.html">Go to the source code of this file.</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="namespaces"></a>
Namespaces</h2></td></tr>
<tr class="memitem:namespacesmlp"><td class="memItemLeft" align="right" valign="top"> &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesmlp.html">smlp</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:acfefb0d57fb5929f1016e9e7dde27272"><td class="memItemLeft" align="right" valign="top">auto&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesmlp.html#acfefb0d57fb5929f1016e9e7dde27272">smlp::sigmoid</a> = [](auto x) { return 1.0f / (1.0f + exp(-x)); }</td></tr>
<tr class="memdesc:acfefb0d57fb5929f1016e9e7dde27272"><td class="mdescLeft">&#160;</td><td class="mdescRight">the sigmoid function is commonly used as the activation function during the forward propagation step. The reason for this is that the sigmoid function maps any input value into a range between 0 and 1, which can be useful for outputting probabilities, among other things. The sigmoid derivative can be expressed in terms of the output of the sigmoid function itself: if σ(x) is the sigmoid function, then its derivative σ'(x) can be computed as σ(x) * (1 - σ(x)).  <a href="namespacesmlp.html#acfefb0d57fb5929f1016e9e7dde27272">More...</a><br /></td></tr>
<tr class="separator:acfefb0d57fb5929f1016e9e7dde27272"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2fc7989c9c78aeacb233301742663e15"><td class="memItemLeft" align="right" valign="top">auto&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesmlp.html#a2fc7989c9c78aeacb233301742663e15">smlp::sigmoidDerivative</a></td></tr>
<tr class="separator:a2fc7989c9c78aeacb233301742663e15"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3947e87343001268acddb73840417c44"><td class="memItemLeft" align="right" valign="top">auto&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesmlp.html#a3947e87343001268acddb73840417c44">smlp::tanhFunc</a> = [](auto x) { return tanh(x); }</td></tr>
<tr class="memdesc:a3947e87343001268acddb73840417c44"><td class="mdescLeft">&#160;</td><td class="mdescRight">Tanh Function (Hyperbolic Tangent): This function is similar to the sigmoid function but maps the input to a range between -1 and 1. It is often used in the hidden layers of a neural network.  <a href="namespacesmlp.html#a3947e87343001268acddb73840417c44">More...</a><br /></td></tr>
<tr class="separator:a3947e87343001268acddb73840417c44"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af62ae3beaa17da55e2d5523e6095c0d7"><td class="memItemLeft" align="right" valign="top">auto&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesmlp.html#af62ae3beaa17da55e2d5523e6095c0d7">smlp::tanhDerivative</a></td></tr>
<tr class="separator:af62ae3beaa17da55e2d5523e6095c0d7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af3bb08506b81f7b7f863a632f4b06b15"><td class="memItemLeft" align="right" valign="top">auto&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesmlp.html#af3bb08506b81f7b7f863a632f4b06b15">smlp::relu</a> = [](auto x) { return std::max(0.0f, x); }</td></tr>
<tr class="memdesc:af3bb08506b81f7b7f863a632f4b06b15"><td class="mdescLeft">&#160;</td><td class="mdescRight">ReLU Function (Rectified Linear Unit): This function outputs the input directly if it’s positive; otherwise, it outputs zero. It has become very popular in recent years because it helps to alleviate the vanishing gradient problem.  <a href="namespacesmlp.html#af3bb08506b81f7b7f863a632f4b06b15">More...</a><br /></td></tr>
<tr class="separator:af3bb08506b81f7b7f863a632f4b06b15"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a442df31244a306fc359abf23d84580e0"><td class="memItemLeft" align="right" valign="top">auto&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesmlp.html#a442df31244a306fc359abf23d84580e0">smlp::reluDerivative</a> = [](auto x) { return x &gt; 0 ? 1.0f : 0.0f; }</td></tr>
<tr class="separator:a442df31244a306fc359abf23d84580e0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a24d45f80e7880fbfc4d246e85a1b863a"><td class="memItemLeft" align="right" valign="top">auto&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesmlp.html#a24d45f80e7880fbfc4d246e85a1b863a">smlp::leakyRelu</a> = [](auto x) { return std::max(0.01f * x, x); }</td></tr>
<tr class="memdesc:a24d45f80e7880fbfc4d246e85a1b863a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Leaky ReLU: This is a variant of ReLU that allows small negative values when the input is less than zero. It can help to alleviate the dying ReLU problem where neurons become inactive and only output zero.  <a href="namespacesmlp.html#a24d45f80e7880fbfc4d246e85a1b863a">More...</a><br /></td></tr>
<tr class="separator:a24d45f80e7880fbfc4d246e85a1b863a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a05fef3a79985b00dab2a7e0e7b4a259c"><td class="memItemLeft" align="right" valign="top">auto&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesmlp.html#a05fef3a79985b00dab2a7e0e7b4a259c">smlp::leakyReluDerivative</a> = [](auto x) { return x &gt; 0 ? 1.0f : 0.01f; }</td></tr>
<tr class="separator:a05fef3a79985b00dab2a7e0e7b4a259c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1d57a311b1145ebfc5a2f605ce948de0"><td class="memItemLeft" align="right" valign="top">auto&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesmlp.html#a1d57a311b1145ebfc5a2f605ce948de0">smlp::parametricRelu</a></td></tr>
<tr class="memdesc:a1d57a311b1145ebfc5a2f605ce948de0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Parametric ReLU (PReLU) is a type of leaky ReLU that, instead of having a predetermined slope like 0.01, learns the slope during training. This can give it a bit more flexibility and help it to learn more complex patterns.  <a href="namespacesmlp.html#a1d57a311b1145ebfc5a2f605ce948de0">More...</a><br /></td></tr>
<tr class="separator:a1d57a311b1145ebfc5a2f605ce948de0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7fbd319755737c46e6b1603547be9fa0"><td class="memItemLeft" align="right" valign="top">auto&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesmlp.html#a7fbd319755737c46e6b1603547be9fa0">smlp::parametricReluDerivative</a></td></tr>
<tr class="separator:a7fbd319755737c46e6b1603547be9fa0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a485c23c19d9a56d4830e3f0a7c9c7291"><td class="memItemLeft" align="right" valign="top">auto&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesmlp.html#a485c23c19d9a56d4830e3f0a7c9c7291">smlp::elu</a></td></tr>
<tr class="memdesc:a485c23c19d9a56d4830e3f0a7c9c7291"><td class="mdescLeft">&#160;</td><td class="mdescRight">the Exponential Linear Units (ELUs) are a great choice as they take on negative values when the input is less than zero, which allows them to push mean unit activations closer to zero like batch normalization. Unlike ReLUs, ELUs have a nonzero gradient for negative input, which avoids the “dead neuron” problem.  <a href="namespacesmlp.html#a485c23c19d9a56d4830e3f0a7c9c7291">More...</a><br /></td></tr>
<tr class="separator:a485c23c19d9a56d4830e3f0a7c9c7291"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a47e24ba887acbadf99fa3dd66a3d2d42"><td class="memItemLeft" align="right" valign="top">auto&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesmlp.html#a47e24ba887acbadf99fa3dd66a3d2d42">smlp::eluDerivative</a></td></tr>
<tr class="separator:a47e24ba887acbadf99fa3dd66a3d2d42"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Activation functions. </p>
<dl class="section author"><dt>Author</dt><dd>Damien Balima (www.dams-labs.net) </dd></dl>
<dl class="section date"><dt>Date</dt><dd>2023-12-02</dd></dl>
<dl class="section copyright"><dt>Copyright</dt><dd>Damien Balima (c) CC-BY-NC-SA-4.0 2023 </dd></dl>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>
